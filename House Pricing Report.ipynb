{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"House Prices: Advanced Regression Techniques\" Kaggle competition\n",
    "\n",
    "## Authors: David FernÃ¡ndez & Rafael Lazcano\n",
    "\n",
    "## Introduction:\n",
    "\n",
    "This document is the final report for the \"Artificial Intelligence and Machine Learning\" subject. It illustrates a whole process of prediction, especifically the challenge of modelling house prices based on a wide set of features. \n",
    "\n",
    "The only rule is to use models covered by the subject's syllabus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleansing\n",
    "\n",
    "Usually data will be available in such a way that cannot be directly processed like empty or incoherent values. The first stage this analysis focuses on transforming the raw data into a suitable dataset while maintining the integrity of the information as much as possible.\n",
    "\n",
    "First, data filling. For most features, like surfaces or distances of characteristics of the house, we suppossed that if a \"na\" is present it is because that specific house lacked that characteristic. For example, a house with \"na\" in \"basement\" surface is considered like does not have basement at all and thus the \"na\" is replaced with a 0. There is an exception for this rule, the \"Year of Garage Building\" for which the median is used to replace the \"na\".\n",
    "\n",
    "Second part of the data preparation is taking care of the categorical variables. First we have to consider all features that are categorical and mark them. Then use the one-hot-encoding on them so they are suitable for processing. Lastly, we have to consider that this enconding creates a new feature for each distinct value, so some columns might exist in the train dataset and not in the test one, or viceversa, so we apply a function to fix it.\n",
    "\n",
    "The functions used are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_values(df):\n",
    "    # Now we fill null-values\n",
    "    df['LotFrontage'].fillna(value=0, inplace=True)\n",
    "    df['MasVnrArea'].fillna(value=0, inplace=True)\n",
    "    df['BsmtFinSF1'].fillna(value=0, inplace=True)\n",
    "    df['BsmtFinSF2'].fillna(value=0, inplace=True)\n",
    "    df['BsmtUnfSF'].fillna(value=0, inplace=True)\n",
    "    df['TotalBsmtSF'].fillna(value=0, inplace=True)\n",
    "    df['BsmtFullBath'].fillna(value=0, inplace=True)\n",
    "    df['GarageArea'].fillna(value=0, inplace=True)\n",
    "    df['GarageCars'].fillna(value=0, inplace=True)\n",
    "    df['GarageArea'].fillna(value=0, inplace=True)\n",
    "    df['BsmtHalfBath'].fillna(value=0, inplace=True)\n",
    "    df['GarageYrBlt'].fillna(df['GarageYrBlt'].median(), inplace=True)\n",
    "    return df\n",
    "\n",
    "def one_hot_encode(df, skip_features=[]):\n",
    "    for col in [\n",
    "                'Alley', 'PoolQC', 'Fence', 'MiscFeature',\n",
    "                'MSSubClass', 'MSZoning', 'Street', 'LotShape', 'MSZoning', 'LandContour', 'Utilities',\n",
    "                'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n",
    "                'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n",
    "                'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical',\n",
    "                'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n",
    "                'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition']:\n",
    "        if col in df.columns and col not in skip_features:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    # Apply one-hot-enconding to all categorical variables.\n",
    "    categorical_columns = df.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    for categoricalVariable in categorical_columns:\n",
    "        dummy = pd.get_dummies(df[categoricalVariable], prefix=categoricalVariable).astype('category')\n",
    "        df = pd.concat([df, dummy], axis=1)\n",
    "        df.drop([categoricalVariable], axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_one_hot_encoded_columns(train_df, test_df):\n",
    "    \"\"\"\n",
    "    After one-hot encoding, some columns might exist in the train dataset and not in the test one, or viceversa.\n",
    "    If a column exists in one of the two dataframes and not in the other, we create it and fill it with zeros.\n",
    "    \"\"\"\n",
    "    clean_test_set = set(list(test_df.columns.values))\n",
    "    clean_train_set = set(list(train_df.columns.values))\n",
    "\n",
    "    differences = list(clean_test_set ^ clean_train_set)\n",
    "    differences.remove('SalePrice')\n",
    "\n",
    "    for dif in differences:\n",
    "        if dif not in train_df:\n",
    "            train_df[dif] = 0\n",
    "            train_df[dif] = train_df[dif].astype('category')\n",
    "        if dif not in test_df:\n",
    "            test_df[dif] = 0\n",
    "            test_df[dif] = test_df[dif].astype('category')\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feature selection and engineering\n",
    "\n",
    "The next stage of the process is the feature selection and engineering. The main goal is to reduce noise on the data by deleting those features that do not have a good correlation with the target variable and also modify those that do in such a way that that correlation is easier to \"understand\" for the prediction model.\n",
    "\n",
    "Taking into consideration that \"a priori\" we dont know what functions will achieve this, the process has 2 stages. The first one is the definition of set of functions that based on stastistical considerations (like features that originally had a lot of \"na's\" contain a lot of noise) and business-based considerations (like knowing that the sum of the surfaces is a better price indicator than explicitly using each surface).\n",
    "\n",
    "You can see the original set of functions here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# FEATURE ENGINEERING FUNCTIONS\n",
    "###############################\n",
    "\n",
    "def sum_SF(df):\n",
    "    columns_to_add = ['1stFlrSF', '2ndFlrSF', 'BsmtFinSF1', 'BsmtFinSF2']\n",
    "    if pd.Series(columns_to_add).isin(df.columns).all():\n",
    "        df['House_SF'] = df[columns_to_add].sum(axis=1)\n",
    "        df.drop(columns_to_add, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def sum_Baths(df):\n",
    "    bath_features = ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']\n",
    "    if pd.Series(bath_features).isin(df.columns).all():\n",
    "        df['Total_Baths'] = (df['FullBath'] +\n",
    "                             df['BsmtFullBath'] +\n",
    "                             (0.8*df['HalfBath']) +\n",
    "                             (0.8*df['BsmtHalfBath']))\n",
    "        df.drop(bath_features, axis=1,inplace = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def sum_Porch(df):\n",
    "    columns_to_add = ['OpenPorchSF','3SsnPorch','EnclosedPorch','ScreenPorch','WoodDeckSF']\n",
    "    if pd.Series(columns_to_add).isin(df.columns).all():\n",
    "        df['Porch_sf'] = df[columns_to_add].sum(axis=1)\n",
    "        df.drop(columns_to_add, axis=1,inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_skewness(df):\n",
    "    numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    numeric_features = []\n",
    "    for i in df.columns:\n",
    "        if df[i].dtype in numeric_dtypes: \n",
    "            numeric_features.append(i)\n",
    "\n",
    "    feature_skew = df[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "    skews = pd.DataFrame({'skew':feature_skew})\n",
    "    return feature_skew, numeric_features\n",
    "\n",
    "\n",
    "def fix_skewness(df):\n",
    "    feature_skew, numeric_features = feature_skewness(df)\n",
    "    high_skew = feature_skew[feature_skew > 0.9]\n",
    "    skew_index = high_skew.index\n",
    "    for i in skew_index:\n",
    "        df[i] = boxcox1p(df[i], boxcox_normmax(df[i]+1))\n",
    "\n",
    "    #skew_features = df[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "    #skews = pd.DataFrame({'skew':skew_features})\n",
    "    return df\n",
    "\n",
    "\n",
    "def categorical_to_ordinal(df):\n",
    "    \"\"\"\n",
    "    Some textual features(e.g.basement quality) should be handled as numerical (i.e.ordinal) values\n",
    "    \"\"\"\n",
    "\n",
    "    ordinal_features = ['ExterQual', 'BsmtQual', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'PoolQC',\n",
    "                        'ExterCond', 'BsmtCond', 'GarageCond']\n",
    "    for ordinalFeature in ordinal_features:\n",
    "        if ordinalFeature in df:\n",
    "            df[ordinalFeature].fillna(value=0, inplace=True)\n",
    "            df[ordinalFeature] = df[ordinalFeature].replace({\n",
    "                                            'Ex': 5,\n",
    "                                            'Gd': 4,\n",
    "                                            'TA': 3,\n",
    "                                            'Fa': 2,\n",
    "                                            'Po': 1,\n",
    "                                            'NA': 0\n",
    "                                            }).astype('int32')\n",
    "    if 'Foundation' in df:\n",
    "        df['Foundation'].fillna(value=0, inplace=True)\n",
    "        df['Foundation'] = df['Foundation'].replace({\n",
    "                                            'PConc': 3,\n",
    "                                            'CBlock': 2,\n",
    "                                            'BrkTil': 1,\n",
    "                                            'Slab': 0,\n",
    "                                            'Stone': 0,\n",
    "                                            'Wood': 0,\n",
    "                                            'NA': 0\n",
    "                                            }).astype('int32')\n",
    "    return df\n",
    "\n",
    "\n",
    "def transform_sales_to_log_of_sales(df):\n",
    "    \"\"\"\n",
    "    Our target values distribution get closer to a normal distribution using the log-transformation\n",
    "    \"\"\"\n",
    "    if 'SalePrice' in df:\n",
    "        df['SalePrice'] = df['SalePrice'].apply(np.log1p)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_expensive_neighborhood_feature(df):\n",
    "    \"\"\"\n",
    "    Instead of using all the neighborhoods, we use a binary classification: are they located in one of the 5 most\n",
    "    expensive neighborhoods?\n",
    "    \"\"\"\n",
    "    expensive_neighborhoods = ['Neighborhood_NoRidge', 'Neighborhood_NridgHt', 'Neighborhood_StoneBr',\n",
    "                               'Neighborhood_Somerst', 'Neighborhood_Crawfor']\n",
    "\n",
    "    for neighborhood in expensive_neighborhoods:\n",
    "        df.loc[df[neighborhood] == 1, \"Expensive_Neighborhood\"] = 1\n",
    "    df[\"Expensive_Neighborhood\"].fillna(0, inplace=True)\n",
    "    df.drop([col for col in df if col.startswith('Neighborhood')], axis=1, inplace=True, errors=\"ignore\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_home_quality(df):\n",
    "    df['HomeQuality'] = df['OverallQual'] + df['OverallCond']\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_years_since_last_remodel(df):\n",
    "    df['YearsSinceLastRemodel'] = df['YrSold'].astype(int) - df['YearRemodAdd'].astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_too_cheap_outliers(df):\n",
    "    new_df = df[df[\"SalePrice\"] > 50000]\n",
    "    if new_df.shape[0] > 500:\n",
    "        return new_df\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "\n",
    "def remove_garage_cars_feature(df):\n",
    "    \"\"\"\n",
    "    'GarageCars' feature is related to GarageArea feature, it might be interesting to remove it\n",
    "    \"\"\"\n",
    "    df.drop(['GarageCars'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_lotfrontage_feature(df):\n",
    "    df.drop(['LotFrontage'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_empty_features(df):\n",
    "    \"\"\"\n",
    "    Drop features 'Alley', 'PoolQC', 'Fence' and 'MiscFeature', which are almost empty\n",
    "    \"\"\"\n",
    "    df.drop(['Alley', 'PoolQC', 'Fence', 'MiscFeature'], axis=1, inplace=True, errors='ignore')\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_under_represented_features(df):\n",
    "    \"\"\"\n",
    "    Eliminate those columns with most of the information belonging to the same class\n",
    "    \"\"\"\n",
    "    under_rep = []\n",
    "    for i in df.columns:\n",
    "        if i != 'SalePrice':\n",
    "            counts = df[i].value_counts()\n",
    "            zeros = counts.iloc[0]\n",
    "            if ((zeros / len(df)) * 100) > 99.0:\n",
    "                under_rep.append(i)\n",
    "    #not_dropped_features = set(df.columns) - set(under_rep)\n",
    "    df.drop(under_rep, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_selection_lasso(df):\n",
    "    \"\"\"\n",
    "    Use Lasso to select the most meaningful features\n",
    "    \"\"\"\n",
    "    clf = linear_model.Lasso(alpha=0.01)\n",
    "    X = df.drop(['SalePrice'], axis=1)\n",
    "    y = df.SalePrice.reset_index(drop=True)\n",
    "    clf.fit(X, y)\n",
    "    zero_indexes = np.where(clf.coef_ == 0)[0]\n",
    "    #not_dropped_features = set(df.columns) - set(zero_indexes)\n",
    "    if len(df.columns) - len(zero_indexes) > 5:\n",
    "        df.drop(X.columns[zero_indexes], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def f_regression_feature_filtering(df):\n",
    "    \"\"\"\n",
    "    Select the 18 best features to the target using f-test regression\n",
    "    (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression)\n",
    "    \"\"\"\n",
    "    X = df.drop(['SalePrice'], axis=1)\n",
    "    y = df.SalePrice.reset_index(drop=True)\n",
    "    best_features_indexes = SelectKBest(k=18, score_func=f_regression).fit(X, y).get_support(indices=True)\n",
    "    filtered_features = df.filter(items=X.columns[best_features_indexes], axis=1)\n",
    "    return filtered_features.join(df.SalePrice)\n",
    "\n",
    "\n",
    "\n",
    "def drop_categories(df):\n",
    "    categorical_columns = df.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    for categoricalVariable in categorical_columns:\n",
    "        if categoricalVariable not in ['ExterQual_Ex', 'ExterQual_Gd', 'ExterQual_TA', 'ExterQual_Fa', 'ExterQual_Po',\n",
    "                                       'KitchenQual_Ex', 'KitchenQual_Gd', 'KitchenQual_TA', 'KitchenQual_Fa',\n",
    "                                       'KitchenQual_Po']:\n",
    "            df.drop([categoricalVariable], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    return df\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward selecting the best function engineering combinations\n",
    "Once the functions are ready, we have to select a group of those (and to which features apply) that leads to the best result of the model.\n",
    "A quick calculation on combinations proves that brute force is not possible in a reasonable time. We will use a technique known as \"forward selection\" instead, this will take us to a local minimum in the search space in resonable time of computation. First, we start with the empty set of functions, and we start iterating over them, training and validating the model. For each iteration we select the 3 best possible combinations, \"fixing\" them for the next iteration. The best possible combinations are kept.\n",
    "\n",
    "\n",
    "In pseudo-code:\n",
    "\n",
    "FUNCTIONS_FROM_PREVIOUS_STEP <- [] FOR _ IN ALL FUNCTIONS: FUNCTIONS FROM CURRENT_STEP <- [] FOR FIXED_FUNCTIONS IN FUNCTIONS_FROM_PREVIOUS_STEP: FOR OTHER_FUNCTION IN (ALL_FUNCTIONS - FIXED_FUNCTIONS): SCORE <- EVALUATE(FIXED_FUNCTIONS + OTHER_FUNCTION) IF LEN(FUNCTIONS_FROM_CURRENT_STEP) < 3 OR SCORE > SCORE(FUNCTIONS_FROM_CURRENT_STEP): ADD FUNCTIONS_FROM_CURRENT_STEP TO FUNCTIONS_FROM_CURRENT_STEP FUNCTIONS_FROM_PREVIOUS_STEP <- FUNCTIONS_FROM_CURRENT_STEP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTIONS_FROM_PREVIOUS_STEP <- [] \n",
    "FOR _ IN ALL FUNCTIONS: FUNCTIONS FROM CURRENT_STEP <- [] \n",
    "    FOR FIXED_FUNCTIONS IN FUNCTIONS_FROM_PREVIOUS_STEP: \n",
    "        FOR OTHER_FUNCTION IN (ALL_FUNCTIONS - FIXED_FUNCTIONS): \n",
    "            SCORE <- EVALUATE(FIXED_FUNCTIONS + OTHER_FUNCTION) \n",
    "            IF LEN(FUNCTIONS_FROM_CURRENT_STEP) < 3 OR SCORE > SCORE(FUNCTIONS_FROM_CURRENT_STEP): \n",
    "                ADD FUNCTIONS_FROM_CURRENT_STEP TO FUNCTIONS_FROM_CURRENT_STEP \n",
    "    FUNCTIONS_FROM_PREVIOUS_STEP <- FUNCTIONS_FROM_CURRENT_STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "functions_kept_per_step = 3\n",
    "functions_kept_from_previous_step = [([],None)]\n",
    "for _ in all_fe_functions:\n",
    "    functions_kept_from_current_step = []\n",
    "    while len(functions_kept_from_previous_step) > 0:\n",
    "        functions_kept = functions_kept_from_previous_step.pop()[0]\n",
    "        other_functions = set(all_fe_functions) - set(functions_kept)\n",
    "        for other_function in other_functions:\n",
    "                functions_to_evaluate = functions_kept + [other_function]\n",
    "\n",
    "                try:\n",
    "                    print(\"\\nStarting functions {}\".format(functions_to_evaluate))\n",
    "                    # Load data set\n",
    "                    train = pd.read_csv('train.csv').set_index('Id')\n",
    "                    test = pd.read_csv('test.csv').set_index('Id')\n",
    "\n",
    "                    # Allow info in bigger dataframes\n",
    "                    pd.options.display.max_info_columns = 350\n",
    "\n",
    "                    # in some cases, there are specific features that we do not want to one-hot encode\n",
    "                    skip_one_hot_encode_features = []\n",
    "                    if 'categorical_to_ordinal' in functions_to_evaluate:\n",
    "                        skip_one_hot_encode_features = ['PoolQC']\n",
    "\n",
    "\n",
    "                    # Data preparation\n",
    "                    clean_train = one_hot_encode(fill_na_values(train))\n",
    "                    clean_test = one_hot_encode(fill_na_values(test))\n",
    "                    clean_train, clean_test = merge_one_hot_encoded_columns(clean_train, clean_test)\n",
    "\n",
    "                    # Feature engineering\n",
    "                    for fe_function in functions_to_evaluate:\n",
    "                        clean_train = globals()[fe_function](clean_train)\n",
    "                        if fe_function in fe_functions_only_for_training_set:\n",
    "                            continue\n",
    "                        elif fe_function in dynamic_feature_selection_functions:\n",
    "                            # some functions remove features dynamically, we need to apply the same changes to the test data set\n",
    "                            clean_test = clean_test[clean_train.drop('SalePrice', axis=1).columns]\n",
    "                        else:\n",
    "                            clean_test = globals()[fe_function](clean_test)\n",
    "\n",
    "                    X = clean_train.loc[:, clean_train.columns != 'SalePrice']\n",
    "                    y = clean_train.loc[:, 'SalePrice']\n",
    "\n",
    "                    # Create linear regression object\n",
    "                    #regr = linear_model.LinearRegression()\n",
    "                    regr = ensemble.GradientBoostingRegressor()\n",
    "\n",
    "                    # The metrics\n",
    "                    #score = r2_score(y_test, y_pred)\n",
    "                    # print(stats.describe(regr.coef_))\n",
    "                    # mse = mean_squared_error(y_test, y_pred)\n",
    "                    # rmse = np.sqrt(mean_squared_error(np.log(y_test), np.log(y_pred)))\n",
    "                    # r2 = r2_score(np.log(y_test), np.log(y_pred))\n",
    "                    # print(\" FUNCTIONS : {}\".format(functions))\n",
    "                    # print(\" sklearn score: {}\".format(regr.score(X_test, y_test)))\n",
    "                    # print(\"r2 {}\".format(r2))\n",
    "                    # print(\"rmse {}\".format(rmse))\n",
    "                    \n",
    "                    scores = [0]\n",
    "                    try:\n",
    "                        scores = cross_val_score(regr, X, y, cv=5, n_jobs=-1)\n",
    "                    score = scores.mean()\n",
    "\n",
    "                    #if we haven't yet kept 3 sets of functions in this step, or this score is better than the third set kept\n",
    "                    if len(functions_kept_from_current_step) < functions_kept_per_step:\n",
    "                        functions_kept_from_current_step.append((functions_to_evaluate, score))\n",
    "                    elif score > functions_kept_from_current_step[2][1]:\n",
    "                        functions_kept_from_current_step[2] = (functions_to_evaluate, score)\n",
    "                    functions_kept_from_current_step.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "\n",
    "                    # keep track of results so far\n",
    "    functions_kept_from_previous_step = functions_kept_from_current_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection and tuning\n",
    "\n",
    "This stage consists on finding the optimal values for the hyperparameters of the models that we are going to use. These models are:\n",
    "\n",
    "* Linear Regression  \n",
    "* Lasso Regression\n",
    "* Ridge Regression\n",
    "* Decision trees\n",
    "* Random Forest\n",
    "* XGBoost\n",
    "\n",
    "Making use of GridSearch and cross-validation, we define a range of values for each hyperparameter of each model and try every combination. Those combinations that result in best performance are saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b11e3ed56acf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \"\"\"\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LassoCV, Lasso, RidgeCV, Ridge\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "lassoCV = LassoCV(alphas=[0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1], cv=5, random_state=1)\n",
    "\n",
    "bestLasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0001, random_state=1))\n",
    "\n",
    "\n",
    "bestLinear = make_pipeline(RobustScaler(), LinearRegression())\n",
    "\n",
    "\n",
    "parameters = {'n_estimators': [500, 1000, 2000, 3000, 5000], \n",
    "              'learning_rate': [0.05, 0.1, 0.5, 1],\n",
    "              'max_depth': [3, 4, 5],\n",
    "              'min_samples_leaf': [5, 10, 15, 20],\n",
    "              'min_samples_split': [2, 5, 7, 10]}\n",
    "\n",
    "GBoost = GridSearchCV(GradientBoostingRegressor(max_features='sqrt', loss='huber', random_state=1), parameters, cv=5)\n",
    "GBoost.fit(X, y)\n",
    "GBoost.best_estimator_\n",
    "\n",
    "\n",
    "bestGBoost = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.05,\n",
    "                                   max_depth=5, max_features='sqrt',\n",
    "                                   min_samples_leaf=10, min_samples_split=7, \n",
    "                                   loss='huber', random_state=1)\n",
    "\n",
    "ridge = RidgeCV(alphas=(0.001, 0.005, 0.1, 0.5, 1))\n",
    "ridge.fit(X, y)\n",
    "ridge.alpha_\n",
    "\n",
    "bestRidge = Ridge(alpha=1)\n",
    "\n",
    "\n",
    "parameters = {'max_depth': [3, 5, 10],\n",
    "              'min_samples_leaf': [5, 10, 15, 20],\n",
    "              'min_samples_split': [2, 5, 7, 10],\n",
    "              'max_features': ['auto', 'sqrt', 'log2']}\n",
    "decisionTree = GridSearchCV(DecisionTreeRegressor(random_state=1), parameters, cv=5)\n",
    "decisionTree.fit(X, y)\n",
    "decisionTree.best_estimator_\n",
    "\n",
    "\n",
    "bestDecisionTree = DecisionTreeRegressor(criterion='mse', max_depth=10, max_features='auto',\n",
    "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "           min_impurity_split=None, min_samples_leaf=15,\n",
    "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "           presort=False, random_state=1, splitter='best')\n",
    "\n",
    "\n",
    "parameters = {\n",
    "              'max_depth': [3, 5, 7, 10, 50, None],\n",
    "              'min_samples_leaf': [5, 10, 15, 20],\n",
    "              'min_samples_split': [2, 5, 7, 10],\n",
    "              'max_features': ['auto', 'sqrt', 'log2']}\n",
    "\n",
    "randomForest = GridSearchCV(RandomForestRegressor(random_state=1), parameters, cv=5)\n",
    "randomForest.fit(X, y)\n",
    "randomForest.best_estimator_\n",
    "\n",
    "bestRandomForest = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=10,\n",
    "           max_features='log2', max_leaf_nodes=None,\n",
    "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "           min_samples_leaf=5, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "           oob_score=False, random_state=1, verbose=0, warm_start=False)\n",
    "\n",
    "parameters = {'max_depth': [1, 3, 5],\n",
    "             'learning_rate': [0.05, 0.1, 0.5, 1],\n",
    "             'gamma': [0.01, 0.05, 0.1],\n",
    "             'min_child_weight': [0.5, 1, 3],\n",
    "             'reg_alpha': [0.1, 0.5, 1],\n",
    "             'reg_lambda': [0.1, 0.5, 1],\n",
    "             'colsample_bytree': [0.1, 0.5, 1]}\n",
    "xgbReg = GridSearchCV(xgb.XGBRegressor(silent=True, nthread=-1, random_state=1), parameters, cv=5)\n",
    "xgbReg.fit(X, y)\n",
    "xgbReg.best_estimator_\n",
    "\n",
    "\n",
    "bestXGB = xgb.XGBRegressor(n_estimators=2500,\n",
    "                           learning_rate=0.05, max_depth=3,\n",
    "                           colsample_bytree=0.5, gamma=0.05, \n",
    "                           min_child_weight=1, reg_alpha=0.5,\n",
    "                           reg_lambda=1, subsample=0.5,\n",
    "                           random_state =1, nthread = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods\n",
    "\n",
    "After last stage we have all and each of the models optimized. One option would be to select the best performing model, but it has been empirically proved that many times, the linear combination with of worse performing models ends up giving better results than the best individual model. \n",
    "\n",
    "That said, our strategy is to try different linear combinations of all the models and test if we can find a combination that gives a better result than the best individual model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bestXGB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-dd8f63e8e599>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbestModels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbestXGB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbestLasso\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbestLinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbestRidge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbestGBoost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbestRidge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbestDecisionTree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbestRandomForest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbestModels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bestXGB' is not defined"
     ]
    }
   ],
   "source": [
    "bestModels = [bestXGB, bestLasso, bestLinear, bestRidge, bestGBoost, bestRidge, bestDecisionTree, bestRandomForest]\n",
    "for model in bestModels:\n",
    "    model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport itertools\\nfrom sklearn.metrics import r2_score\\nweights = [0, 0.1, 0.5, 1] \\n\\nall_results = []\\nfor weights in itertools.product(weights, repeat=len(bestModels)):\\n    if np.sum(weights) == 0 :\\n        continue\\n    predictions = np.column_stack([\\n        model.predict(X_test) for model in bestModels\\n    ])\\n    weighted_predictions = predictions * weights\\n    summed_predictions = np.sum(weighted_predictions, axis=1)\\n    result_predictions = summed_predictions / np.sum(weights)\\n    score = r2_score(y_test, result_predictions)\\n    all_results.append((weights, score))\\n    print(weights)\\n\\nall_results.sort(key=lambda tup: tup[1], reverse=True)\\nall_results\\n'"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import itertools\n",
    "from sklearn.metrics import r2_score\n",
    "weights = [0, 0.1, 0.5, 1] \n",
    "\n",
    "all_results = []\n",
    "for weights in itertools.product(weights, repeat=len(bestModels)):\n",
    "    if np.sum(weights) == 0 :\n",
    "        continue\n",
    "    predictions = np.column_stack([\n",
    "        model.predict(X_test) for model in bestModels\n",
    "    ])\n",
    "    weighted_predictions = predictions * weights\n",
    "    summed_predictions = np.sum(weighted_predictions, axis=1)\n",
    "    result_predictions = summed_predictions / np.sum(weights)\n",
    "    score = r2_score(y_test, result_predictions)\n",
    "    all_results.append((weights, score))\n",
    "    print(weights)\n",
    "\n",
    "all_results.sort(key=lambda tup: tup[1], reverse=True)\n",
    "all_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final prediction\n",
    "\n",
    "After all these stages we found that there are several combinations of models that outperform the indivdual best model. Among all those posibilities, the one that works best is composed of: \n",
    "* XGBoost\n",
    "* Ridge (in a small percentage)\n",
    "* GBoost\n",
    "* Random Forest\n",
    "\n",
    "As you can see, it relies on the XGBoost more than in any other. We could say that our final model is an XGBoost with a second order aproximation using Ridge,GBoost and Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModels = [bestXGB, bestLasso, bestLinear, bestRidge, bestGBoost, bestRidge, bestDecisionTree, bestRandomForest]\n",
    "for model in bestModels:\n",
    "    model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights = (2, 0, 0, 0.1, 1.1, 0.1, 0, 0.1)\n",
    "predictions = np.column_stack([\n",
    "        model.predict(clean_test) for model in bestModels\n",
    "    ])\n",
    "weighted_predictions = predictions * best_weights\n",
    "summed_predictions = np.sum(weighted_predictions, axis=1)\n",
    "result_predictions = summed_predictions / np.sum(best_weights)\n",
    "\n",
    "result_predictions = np.expm1(result_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_test['SalePrice'] = result_predictions\n",
    "submission = clean_test[['SalePrice']]\n",
    "submission.to_csv('stacked.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
